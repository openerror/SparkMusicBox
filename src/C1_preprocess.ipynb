{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Recommendation System - Data Preprocessing\n",
    "## Design strategy \n",
    "The act of downloading itself may not signal 'true' interest. But when combined with plays (the more the merrier), it is a smoking gun indicating strong preference. Therefore, we will form a utility matrix mainly based on play data, and add bonus values to those user/song pairs where there are also downloads.\n",
    "\n",
    "To be clear, we would only recommend songs for users who have played *something* via streaming. For cold starters, use content-based recommendation instead (which is out of the scope of this data set and project).\n",
    "\n",
    "Moreover, we will only base our recommendations on the **last 30 days of available data**. The assumption is that recent data is a better indication of user preference. From usage experience, Youtube has in fact adopted a similar strategy; individualized recommedations on the front page are reset after ~2 weeks of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark session, in case not invoking from CMD as 'pyspark'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"data_cleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = spark.read.format('csv').options(header='true', inferSchema='true').load(\"../data/play_ds_cleaned.csv\").cache()\n",
    "down = spark.read.format('csv').options(header='true', inferSchema='true').load(\"../data/down_ds.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast date to correct type, for easier manipulation\n",
    "play = play.withColumn('date', F.col('date').cast('date'))\n",
    "play = play.withColumn('song_id', F.col('song_id').cast('int'))\n",
    "down = down.withColumn('date', F.col('date').cast('date'))\n",
    "\n",
    "# play_time can't be in str form!\n",
    "play = play.withColumn('play_time', F.col('play_time').cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict the time window considered\n",
    "\n",
    "Idea: recent data is better indicator of *current* user preference. We will take the recent 30 days, for starters. Later on in `B2_compute_score.ipynb` we will further divide the data up by date into training and test sets.\n",
    "\n",
    "Filter early to reduce data size and speeds up computations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| max(date)|\n",
      "+----------+\n",
      "|2017-05-12|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "| max(date)|\n",
      "+----------+\n",
      "|2017-05-12|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check last available date - both are May 12!\n",
    "play.select('date').agg(F.max('date')).show()\n",
    "down.select('date').agg(F.max('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "from dateutil import parser\n",
    "\n",
    "lookback_window_size = 30\n",
    "last_date = parser.parse('2017-05-12').date()\n",
    "\n",
    "play = play.filter(F.col('date') >= (last_date - datetime.timedelta(lookback_window_size)))\n",
    "down = down.filter(F.col('date') >= (last_date - datetime.timedelta(lookback_window_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| min(date)|\n",
      "+----------+\n",
      "|2017-04-12|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "| min(date)|\n",
      "+----------+\n",
      "|2017-04-12|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm filtering\n",
    "play.select('date').agg(F.min('date')).show()\n",
    "down.select('date').agg(F.min('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: int, device: string, song_id: int, date: date, play_time: int, song_length: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column names/types\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: int, device: string, song_id: int, date: date]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column names/types\n",
    "down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused column; not distinguishing between device types when recommending\n",
    "play = play.drop('device')\n",
    "down = down.drop('device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle extreme `song_length`\n",
    "\n",
    "There are observations where `song_length` is either shorter than 5 secs, or longer than 200 hours. Reasonable to suspect that the data itself is faulty --- because a simple search reveals other observations 1) with the same `song_id`; 2) but with \"normal\" `song_length`.\n",
    "\n",
    "Strategy:\n",
    "1. Define \"normal\" `song_length` as 1 secs <= `song_length` <= 80 mins. 80 mins is the longest possible length of a CD album. As for the lower bound...maybe some people do need to find really short tracks, so let's allow for that.\n",
    "2. For songs (`song_id`) that actually have normal `song_length` elsewhere in the data, compute an avarage of those normal values, and impute the entries where `song_length` is anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|min(song_length)|\n",
      "+----------------+\n",
      "|            -1.0|\n",
      "+----------------+\n",
      "\n",
      "+----------------+\n",
      "|max(song_length)|\n",
      "+----------------+\n",
      "|        776363.0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the extremes of song_length\n",
    "play.select('song_length').agg(F.min('song_length')).show()\n",
    "play.select('song_length').agg(F.max('song_length')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71549\n",
      "299114\n"
     ]
    }
   ],
   "source": [
    "# Count distinct song_id with *any* anomalous song_length\n",
    "anomaly_sid = play.filter( (F.col('song_length') < 1) | (F.col('song_length') > 80*60) ).select('song_id').distinct()\n",
    "print(anomaly_sid.count())\n",
    "\n",
    "# Count distinct song_id with *any* normal song_length\n",
    "normal_sid = play.filter( (F.col('song_length') >= 1) & (F.col('song_length') <= 80*60) ).select('song_id').distinct()\n",
    "print(normal_sid.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54487"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# song_id's common to both sets above\n",
    "# ~80% of anomaly_sid.count()!\n",
    "anomaly_sid.select('song_id').intersect(normal_sid.select('song_id')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------+\n",
      "| song_id|count(DISTINCT song_length)|\n",
      "+--------+---------------------------+\n",
      "|21965412|                         13|\n",
      "| 2997638|                          2|\n",
      "| 9541609|                          1|\n",
      "| 4795040|                          1|\n",
      "| 6105580|                          1|\n",
      "+--------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate necessity of averaging: same song_id can correspond to multiple song_length\n",
    "play.select('song_id', 'song_length').groupby('song_id')\\\n",
    "    .agg(F.countDistinct('song_length')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average song_length for each of normal_sid (song_id) \n",
    "averaged_lengths = play.select('song_id', 'song_length')\\\n",
    "                        .filter( (F.col('song_length') >= 1) & (F.col('song_length') <= 80*60) )\\\n",
    "                        .groupby('song_id')\\\n",
    "                        .agg(F.mean('song_length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with existing play data!\n",
    "play = play.join(averaged_lengths, on='song_id', how='left')\n",
    "\n",
    "# Fill gaps in avg(song_length) with existing song_length\n",
    "play = play.withColumn(\"avg(song_length)\", \n",
    "                       F.when(F.col('avg(song_length)').isNull(), \n",
    "                              F.col('song_length')).otherwise(F.col('avg(song_length)'))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the now redundant information\n",
    "play = play.drop('song_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------+\n",
      "|avg(avg(song_length))|stddev_samp(avg(song_length))|\n",
      "+---------------------+-----------------------------+\n",
      "|    267.2323546138402|           373.62636500890176|\n",
      "+---------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine statistics of the new feature; seems reasonable...\n",
    "play.select('avg(song_length)').agg(F.mean('avg(song_length)'), \n",
    "                                    F.stddev('avg(song_length)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12091.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Much longer than the length considered 'normal'\n",
    "play.approxQuantile(\"avg(song_length)\", [0.5], 0.2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set null values in avg(song_length), if any, to avg (Not using median because it's 20 hours)\n",
    "# It would take too much additional data processing to utilize fancier imputation methods\n",
    "# e.g. encoding the date...train K-means model...\n",
    "\n",
    "# Copied from above\n",
    "song_length_avg = 267.2323546138402\n",
    "\n",
    "play = play.withColumn('avg(song_length)', \n",
    "                       F.when(F.col('avg(song_length)').isNull(), song_length_avg)\\\n",
    "                       .otherwise(F.col('avg(song_length)'))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006739115838407458\n"
     ]
    }
   ],
   "source": [
    "# Last but not least, drop the rows with anolamous song_length\n",
    "# Measure proportion of data thus affected: ~0.6%. Ok no problem.\n",
    "print(play.filter((F.col('avg(song_length)') > 80.*60.) | (F.col('avg(song_length)') < 1.0)).count() \\\n",
    "      / play.count())\n",
    "\n",
    "play = play.filter((F.col('avg(song_length)') <= 80.*60.) & (F.col('avg(song_length)') >= 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with missing `song_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in raw data:\n",
      "+-------+---+----+---------+----------------+\n",
      "|song_id|uid|date|play_time|avg(song_length)|\n",
      "+-------+---+----+---------+----------------+\n",
      "|   1434|  0|   0|      596|               0|\n",
      "+-------+---+----+---------+----------------+\n",
      "\n",
      "+---+-------+----+\n",
      "|uid|song_id|date|\n",
      "+---+-------+----+\n",
      "|  0|     72|   0|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Demonstrate that there are missing values in song_id\n",
    "    Must drop observations missing song_id, because they are useless for making recommendations,\n",
    "    which involves associating songs with users\n",
    "'''\n",
    "\n",
    "print(\"Missing values in raw data:\")\n",
    "for df in [play, down]:\n",
    "    df.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()\n",
    "    \n",
    "play = play.dropna(how='any', subset=['song_id'])\n",
    "down = down.dropna(how='any', subset=['song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after dropping NAs in song_id:\n",
      "+-------+---+----+---------+----------------+\n",
      "|song_id|uid|date|play_time|avg(song_length)|\n",
      "+-------+---+----+---------+----------------+\n",
      "|      0|  0|   0|      419|               0|\n",
      "+-------+---+----+---------+----------------+\n",
      "\n",
      "+---+-------+----+\n",
      "|uid|song_id|date|\n",
      "+---+-------+----+\n",
      "|  0|      0|   0|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm results of dropping NAs in song_id\n",
    "# Notice how there are less NAs in play_time now\n",
    "# So some observations were missing both song_id and play_time!\n",
    "\n",
    "print(\"Missing values after dropping NAs in song_id:\")\n",
    "for df in [play, down]:\n",
    "    df.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing `play_time`\n",
    "There are entries marked explicitly as NA, and then there are the ones marked '0.0'. The latter is in fact a stand-in for null values. We will have to deal with both.\n",
    "\n",
    "For the former, we will compute the median `play_time` for each corresponding `song_id`; hopefully that covers most of the missing values. Then, we will impute the latter (whatever that remains) using the global median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_missing_play_time = play.filter(F.col('play_time').isNull()).select('song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average song_length for each of normal_sid (song_id) \n",
    "def median(values_list):\n",
    "    med = np.median(values_list)\n",
    "    return float(med)\n",
    "udf_median = F.udf(median, FloatType())\n",
    "\n",
    "median_play_time = play.select('song_id', 'play_time')\\\n",
    "                        .filter( (~F.col('play_time').isNull()) & (F.col('song_id').isin(*songs_missing_play_time)) )\\\n",
    "                        .groupby('song_id')\\\n",
    "                        .agg(udf_median(F.collect_list(F.col('play_time'))))\n",
    "\n",
    "# Rename for easy reference\n",
    "median_play_time = median_play_time.withColumnRenamed('median(collect_list(play_time, 0, 0))',\n",
    "                                                      'median_play_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = play.join(median_play_time, how='left', on='song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain median_value where play_time was null\n",
    "# Replace with existing value otherwise\n",
    "play = play.withColumn('median_play_time',\n",
    "                       F.when(F.col('play_time').isNull(), F.col('median_play_time'))\\\n",
    "                       .otherwise(F.col('play_time'))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+---------+----------------+----------------+\n",
      "|song_id|uid|date|play_time|avg(song_length)|median_play_time|\n",
      "+-------+---+----+---------+----------------+----------------+\n",
      "|      0|  0|   0|      419|               0|              32|\n",
      "+-------+---+----+---------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are still 32 NAs in play_time!\n",
    "play.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in play.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute (approximate) global median of play_time\n",
    "global_median_play_time = play.select('play_time').approxQuantile('play_time', [0.5], 0.1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.0\n"
     ]
    }
   ],
   "source": [
    "# 4 mins and 8 seconds --- reasonable when most tracks are likely to be pop songs!\n",
    "print(global_median_play_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global median and impute remaining missing values with it\n",
    "# global_median_play_time = play.select('play_time').approxQuantile('play_time', [0.5], 0.1)[0]\n",
    "play = play.withColumn('play_time',\n",
    "                      F.when((F.col('play_time')==0.0) | (F.col('play_time').isNull()), global_median_play_time)\\\n",
    "                       .otherwise(F.col('play_time'))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|sum(CAST((play_time IS NULL) AS INT))|\n",
      "+-------------------------------------+\n",
      "|                                    0|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm completion of imputation!\n",
    "play.select(F.sum(F.col('play_time').isNull().cast('int'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save song_ids and uids that're relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs = play.select('song_id').union(down.select('song_id'))\n",
    "# users = play.select('uid').union(down.select('uid'))\n",
    "\n",
    "# # MUST run distinct(), because the union method above preserves duplicates\n",
    "# songs = songs.distinct()\n",
    "# users = users.distinct()\n",
    "\n",
    "# # Save to file\n",
    "# songs.toPandas().to_csv(\"../data/rec/rec_songs.csv\", index=False)\n",
    "# users.toPandas().to_csv(\"../data/rec/rec_users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering intermediate features\n",
    "\n",
    "For each user, we want\n",
    "1. Number of plays per song, throughout the 30 days\n",
    "2. Number of downloads per song, ditto\n",
    "3. Ratio of play length / song length, ditto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plays and downloads per song, per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_per_song = play.groupby('uid', 'song_id').count()\n",
    "downloads_per_song = down.groupby('uid', 'song_id').count()\n",
    "\n",
    "# Rename to faciliate joining later\n",
    "plays_per_song = plays_per_song.withColumnRenamed('count', 'p_count')\n",
    "downloads_per_song = downloads_per_song.withColumnRenamed('count', 'd_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "|uid|song_id|p_count|\n",
      "+---+-------+-------+\n",
      "|  0|      0|      0|\n",
      "+---+-------+-------+\n",
      "\n",
      "+---+-------+-------+\n",
      "|uid|song_id|d_count|\n",
      "+---+-------+-------+\n",
      "|  0|      0|      0|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm that there are no missing values\n",
    "plays_per_song.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in plays_per_song.columns)).show()\n",
    "downloads_per_song.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in downloads_per_song.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio of play length / song length\n",
    "\n",
    "From exploring the data, several issues emerged; they must be solved before the titular feature can be computed.\n",
    "- `play_time` may exceed `song_length`, sometimes by a lot (!)\n",
    "\n",
    "Here are the remedies taken\n",
    "- In lieu of further information, will assume that the entire song is played (i.e. ratio 1)\n",
    "\n",
    "We will demonstrate the issues below, and tackle them one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+\n",
      "|song_id|play_time|  avg(song_length)|\n",
      "+-------+---------+------------------+\n",
      "|   1645|    274.0|272.82608695652175|\n",
      "|   1645|    289.0|272.82608695652175|\n",
      "|   1645|    317.0|272.82608695652175|\n",
      "|   1645|    305.0|272.82608695652175|\n",
      "|   1645|    278.0|272.82608695652175|\n",
      "+-------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+---------+------------------+\n",
      "|song_id|play_time|  avg(song_length)|\n",
      "+-------+---------+------------------+\n",
      "| 124743|    248.0|              84.0|\n",
      "| 133948|  60160.0|             233.0|\n",
      "| 235318|  48540.0|219.35714285714286|\n",
      "| 235318|   3252.0|219.35714285714286|\n",
      "| 235318|   6360.0|219.35714285714286|\n",
      "+-------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Illustrating how play_time > song_length sometimes\n",
    "    Unfortunate anomalies: e.g. for song 1645 below, there are various play_time, some of which\n",
    "    may not actually be the whole song. But in the current scheme, the whole song is *assumed* to be played\n",
    "    \n",
    "    Note that 1645's song_length isn't an integer; it's probably an imputed value. To avoid the anaomaly described above,\n",
    "    we should start from making sure that 1) the logging system functions properly; 2) we have metadata for as many\n",
    "    songs as possible.\n",
    "'''\n",
    "\n",
    "play.filter(F.col('play_time') > F.col('avg(song_length)')).select('song_id', 'play_time', 'avg(song_length)').show(5)\n",
    "\n",
    "play.filter(F.col('play_time') > 2*F.col('avg(song_length)')).select('song_id', 'play_time', 'avg(song_length)').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each play event, compute the ratio of played length over total song length\n",
    "# In case song_length == 0, set the ratio to 0 also\n",
    "\n",
    "play_over_song_length = play.withColumn('ps_ratio', \n",
    "                                        F.when(F.col('play_time') < F.col('avg(song_length)'), F.col('play_time')/F.col('avg(song_length)'))\\\n",
    "                                        .otherwise(1.0))\n",
    "\n",
    "play_over_song_length = play_over_song_length.select('uid', 'song_id', 'ps_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------+-----------------+\n",
      "|round(min(ps_ratio), 4)|max(ps_ratio)|    avg(ps_ratio)|\n",
      "+-----------------------+-------------+-----------------+\n",
      "|                 2.0E-4|          1.0|0.742947795602251|\n",
      "+-----------------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify integrity; and what is the average anyways?\n",
    "play_over_song_length.agg(F.round(F.min('ps_ratio'),4), F.max('ps_ratio'), F.mean('ps_ratio')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Last but not least, consider the fact that there are many rows\n",
    "    with the same (uid, song_id) in play_over_song_length. However,\n",
    "    to build our utility matrix, we can only have one value.\n",
    "    \n",
    "    For simplicity, for each (uid, song_id) pair, we will take the average \n",
    "    ps_ratio. A more elaborate (and probably better) method would be to take \n",
    "    an average weighted by event date --- more recent, higher the weight.\n",
    "'''\n",
    "\n",
    "play_over_song_length = play_over_song_length.groupby('uid', 'song_id').agg(F.mean('ps_ratio'))\n",
    "play_over_song_length = play_over_song_length.withColumnRenamed(\"avg(ps_ratio)\", \"ps_ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join features into one DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left-join from plays_per_song,\n",
    "# to ensure that we keep and prioritize songs that have been played\n",
    "events_per_song = plays_per_song.join(downloads_per_song, how='left', on=['uid', 'song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+\n",
      "|uid|song_id|p_count|d_count|\n",
      "+---+-------+-------+-------+\n",
      "|  0|      0|      0|1646084|\n",
      "+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_per_song.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in events_per_song.columns)).show()\n",
    "\n",
    "# Fill missing d_count with 0; \n",
    "# NAs arise because that particular song has not been downloaded by that particular user\n",
    "events_per_song = events_per_song.fillna(0, subset=['d_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_features = events_per_song.join(play_over_song_length, on=['uid', 'song_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+------------------------------------+\n",
      "|sum(CAST((uid IS NULL) AS INT))|sum(CAST((song_id IS NULL) AS INT))|sum(CAST((p_count IS NULL) AS INT))|sum(CAST((d_count IS NULL) AS INT))|sum(CAST((ps_ratio IS NULL) AS INT))|\n",
      "+-------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+------------------------------------+\n",
      "|                              0|                                  0|                                  0|                                  0|                                   0|\n",
      "+-------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for NAs --- none, which is perfect!\n",
    "rec_features.select(*( F.sum(F.col(c).isNull().cast('int')) for c in rec_features.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save intermediate features to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_features.coalesce(1).write\\\n",
    "            .format(\"com.databricks.spark.csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .save(\"../data/rec/rec_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777079"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777079"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_features.select('uid', 'song_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: int, song_id: int, p_count: bigint, d_count: bigint, ps_ratio: double]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
