{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Recommendataion System, Powered by `Implicit`\n",
    "In the downsampled regime, a laptop with sufficient RAM can train and run a recommendation model. That is what we are going to do now, before considering using the cloud. (See `cloud_train.py` for that.)\n",
    "\n",
    "The package used here, [`implicit`](https://github.com/benfred/implicit), implements the same ALS algorithm as Spark; both cite the [same paper](https://ieeexplore.ieee.org/document/4781121). Therefore, any work done --- cross-validation, in particular --- with [`implicit`](https://github.com/benfred/implicit) can (hopefully) inform that performed using Spark ALS. It is much easier and possibly quicker to iterate models on a single machine. \n",
    "\n",
    "#### Executive summary of results\n",
    "While the recommendation system is performing much better than random guessing (AUC >= 0.75), suggesting songs based on their \"popularity\" among all users may be just as --- if not more --- effective (AUC ~- 0.98). This is not entirely surprising, considering how trends come and go in (popular) music. Why/how are we using the AUC? See the end this notebook for cross-validation attempts and a detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The protagonist today\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# For creating utility matrix\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "\n",
    "# Always beautify\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data and turn into Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load computed rating data\n",
    "df = pd.read_csv(\"../data/rec/upload.csv\", names=['uid', 'song_id', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>song_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12333</td>\n",
       "      <td>15249349</td>\n",
       "      <td>0.298602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1164092</td>\n",
       "      <td>4594934</td>\n",
       "      <td>0.152269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1863042</td>\n",
       "      <td>4090645</td>\n",
       "      <td>0.721931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1863042</td>\n",
       "      <td>7048351</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3629720</td>\n",
       "      <td>4681151</td>\n",
       "      <td>0.398724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid   song_id    rating\n",
       "0    12333  15249349  0.298602\n",
       "1  1164092   4594934  0.152269\n",
       "2  1863042   4090645  0.721931\n",
       "3  1863042   7048351  0.400000\n",
       "4  3629720   4681151  0.398724"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = np.sort(df.uid.unique())\n",
    "songs = df.song_id.unique()\n",
    "ratings = df.rating.values\n",
    "\n",
    "# Get the associated row indices\n",
    "rows = df.uid.astype(CategoricalDtype(categories=users)).cat.codes\n",
    "\n",
    "# Get the associated column indices\n",
    "cols = df.song_id.astype(CategoricalDtype(categories=songs)).cat.codes\n",
    "\n",
    "# Floating point accuracy not likely a major concern; use float32 instead 64 to speed up computations\n",
    "utility_sparse = sparse.csr_matrix((ratings, (rows, cols)), \n",
    "                                   shape = (len(users), len(ratings)), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99648765410417\n"
     ]
    }
   ],
   "source": [
    "# Measure sparsity --- 99.9! Use of csr_matrix justified\n",
    "\n",
    "matrix_size = utility_sparse.shape[0]*utility_sparse.shape[1] # Number of possible interactions in the matrix\n",
    "num_ratings = len(ratings) # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_ratings/matrix_size))\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Test Sets via Masking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain masking here\n",
    "\n",
    "Function `make_train` adopted from the blog of [Jesse Steinweg-Woods](https://jessesw.com/Rec-System/`). Made minor edits to \n",
    "1. Remove unnecessary casting to `int` after ceil op\n",
    "2. Use Python's built-in ceil instead of np.ceil; former is quicker when working on a single scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "\n",
    "def make_train(ratings, pct_test = 0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix. \n",
    "    \n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the \n",
    "    training set for later comparison to the test set, which contains all of the original ratings. \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs \n",
    "    that originally had interaction set back to zero.\n",
    "    \n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order \n",
    "    compares with the actual interactions.\n",
    "    \n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    \n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    \n",
    "    num_samples = ceil(pct_test*len(nonzero_pairs)) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    \n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERY sparse matrix to begin with; mask 0.10 instead of the 0.20 default\n",
    "train, test, altered_users = make_train(utility_sparse, pct_test=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Training and Cross-Validating\n",
    "\n",
    "The original blog post by [Jesse Steinweg-Woods](https://jessesw.com/Rec-System/`) utilized AUC as the metric for model performance; in short, ... \n",
    "\n",
    "However, Steinweg-Woods did not implement cross-validation routines, which are necessary for choosing the 'best' rank, regularization strength, and number of ALS iterations in a production system. That --- is my contribution here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    J. S.-W.‘s FUNCTIONS FOR COMPUTING AUC\n",
    "\n",
    "    - Bug fixes and optimizations by Everest Law, for correct indexing behavior\n",
    "    - Optimization: Use getrow() instead of indexing, saves ~20 microseconds per op. Significant when there >= 10^4 users.\n",
    "    - Optimization: Preallocate arrays storing AUCs\n",
    "'''\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "        This simple function will output the area under the curve using sklearn's metrics. \n",
    "        - predictions: your prediction output\n",
    "        - test: the actual target result you are comparing to\n",
    "    \n",
    "        Returns AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)   \n",
    "\n",
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model \n",
    "    \n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one. \n",
    "    \n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "    \n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "    \n",
    "    store_auc = np.zeros(shape=(len(altered_users),), dtype='float32')\n",
    "    popularity_auc = np.zeros_like(store_auc)\n",
    "    \n",
    "#     store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "#     popularity_auc = [] # To store popular AUC scores\n",
    "    \n",
    "    \n",
    "    pop_items = np.array(test_set.sum(axis = 0)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for ix, user in enumerate(altered_users): # Iterate through each user that had an item altered        \n",
    "        training_row = training_set.getrow(user).toarray().reshape(-1) # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        \n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        \n",
    "        # Fixed line; see commented line for original\n",
    "        pred = item_vecs.dot( user_vec )[zero_inds]\n",
    "        #pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        \n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set.getrow(user).toarray()[0, zero_inds].reshape(-1)\n",
    "        \n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training \n",
    "        pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        \n",
    "        store_auc[ix] = auc_score(pred, actual) # Calculate AUC for the given user and store\n",
    "        popularity_auc[ix] = auc_score(pop, actual) # Calculate AUC using most popular and score\n",
    "        \n",
    "#         store_auc.append(auc_score(pred, actual)) \n",
    "#         popularity_auc.append(auc_score(pop, actual)) \n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "   # Return the mean AUC rounded to three decimal places for both test and popularity benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVEREST LAW'S OWN FUNCTIONS \n",
    "\n",
    "import itertools\n",
    "\n",
    "def train_model(data, factors=40, regularization=0.1, iterations=30, alpha=15):\n",
    "    '''\n",
    "        Train a SINGLE model using matrix 'data', according to hyperparameters\n",
    "        specified in func arguments\n",
    "        \n",
    "        ** ARGUMENTS **\n",
    "        data: csr_sparse matrix containing ratings; each row is a user, each col is an item\n",
    "        factors: rank of decomposition\n",
    "        regularization: L2 reg factor\n",
    "        iterations: run ALS this many times\n",
    "        alpha:\n",
    "        \n",
    "        Returns trained model, and two sets of vectors (NumPy arrays)\n",
    "    '''\n",
    "    \n",
    "    # Take transpose of `data` because ALS expects cols (not rows) to represent users\n",
    "    model = AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)    \n",
    "    model.fit( data.T * alpha )\n",
    "    \n",
    "    return model, model.user_factors, model.item_factors\n",
    "\n",
    "def cross_validate(training_set, test_set, altered_users, factors, regularizations, iterations, alphas):\n",
    "    '''\n",
    "        For each set of hyperparameters, evaluate model performance (AUC).\n",
    "        Forms the cartesian product of all supplied hyperparameter values, and\n",
    "        then train a model on each of combination. Returns\n",
    "        \n",
    "        ** ARGUMENTS **\n",
    "        factors, regularizations, iterations, alphas: see train_model() for meaning;\n",
    "        MUST BE ITERABLES so that itertools.product can combine them together.\n",
    "        \n",
    "        Returns:\n",
    "    '''\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for cFactor, cReg, cIter, cAlpha in itertools.product(factors, regularizations, iterations, alphas):\n",
    "        print(\"cFactor: {}, cReg: {}, cIter: {}, cAlpha: {}\".format(cFactor, cReg, cIter, cAlpha))\n",
    "        _, user_vecs, item_vecs = train_model(training_set, cFactor, cReg, cIter, cAlpha)\n",
    "        \n",
    "        rec_auc, most_popular_auc = calc_mean_auc(training_set, \n",
    "                                                  altered_users, [user_vecs, item_vecs], test_set)\n",
    "                \n",
    "        # Dict follows argument naming of train_model; facilitates reuse like this: train_model(**params)\n",
    "        params = {\"factors\": cFactor, \"regularization\": cReg, \"iterations\": cIter, \"alpha\": cAlpha}\n",
    "        cv_results.append((params, rec_auc, most_popular_auc))\n",
    "        \n",
    "        # CV might take forever; print latest outcome to monitor process \n",
    "        print(cv_results[-1])\n",
    "        \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validate Matrix Factorization!\n",
    "\n",
    "Here, I try to measure how well the recommendation system recalls songs masked from the training set. This can be thought of as a binary classification problem: the recommendation system generates confidence values for whether there will be activity between the masked pairs of users and songs; then, we calculate an **AUC** based on the confidence values and the test set. Remember that the test set is a replicate of the original data, but binarized into 1's (0's) where there are (aren't) ratings, i.e. activity.\n",
    "\n",
    "As stated in [scikit-learn's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score), the scale of the confidence values doesn't matter when calculating the AUC. Therefore, we can use the model's predictions as they are, without converting them to e.g. probability measures.\n",
    "\n",
    "As a **baseline comparison**, I have also computed an AUC using the masked songs' \"popularity\" and the test set. \"Popularity\" of each song is calculated simply by summing the corresponding column in the original data. The more plays/downloads there were, the higher the \"popularity\". As it turns out, it is hard to beat this baseline where **AUC == 0.981**.\n",
    "\n",
    "Last but not least, the performance of `implicit` is impressive indeed --- 2 seconds per iteration, which is much, much quicker than Spark on Google Cloud. Probably due to communication overhead of the latter? Seriousy recommend using `implicit` (which can be further sped up via CUDA!) when there isn't too much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cFactor: 10, cReg: 0.01, cIter: 5, cAlpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.0/5 [00:09<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'factors': 10, 'regularization': 0.01, 'iterations': 5, 'alpha': 0.1}, 0.789, 0.981)\n",
      "cFactor: 10, cReg: 0.01, cIter: 5, cAlpha: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.0/5 [00:10<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'factors': 10, 'regularization': 0.01, 'iterations': 5, 'alpha': 1.0}, 0.816, 0.981)\n",
      "cFactor: 10, cReg: 0.01, cIter: 5, cAlpha: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.0/5 [00:12<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# factors = [10]\n",
    "# regularizations = [0.01, 0.1, 1.0]\n",
    "# iterations = [1]\n",
    "# alphas = [0.1, 1.0, 10.0]\n",
    "\n",
    "factors = [10, 20]\n",
    "regularizations = [0.01, 0.1]\n",
    "iterations = [5,10]\n",
    "alphas = [0.1,1.0,10]\n",
    "\n",
    "cv_results = cross_validate(train, test, altered_users, factors, regularizations, iterations, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
